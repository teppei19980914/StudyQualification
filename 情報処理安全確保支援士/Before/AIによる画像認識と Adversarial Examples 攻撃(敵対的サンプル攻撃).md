# AIによる画像認識と Adversarial Examples 攻撃(敵対的サンプル攻撃)

## はじめに

2025年10月のプロジェクトマネージャ試験受験を終え、2026年春の情報処理安全確保支援士に向けて勉強中です。  
本記事を含めた各知識のインデックスや学習の道のりについては、「[情報処理安全確保支援士への道のり(随時更新中)](https://qiita.com/teppei19980914/items/6411cb70f2937cbefdcc)」をご参照ください。  
**本記事は学習した内容を記載しています。**  

## 該当問題

[情報処理安全確保支援士令和3年秋期 午前Ⅱ 問1 ](https://www.sc-siken.com/kakomon/03_aki/am2_1.html)  

## AIによる画像認識の基礎

### 画像認識とは

画像認識とは、**画像データを入力として、物体/人物/特徴を自動的に識別/分類する技術**のことです。  
代表的な応用例：  

- 顔認証
- 自動運転(歩行者/信号認識)
- 医療画像診断
- 監視カメラ解析

### 技術的背景

現代の画像認識は主に以下を用います。  

- 深層学習(Deep Learning)
- 畳み込みニューラルネットワーク(CNN)

#### CNNの特徴

- 画像の「局所的特徴(エッジ/模様)」を学習
- 層を重ねることで高次特徴を抽出
- 人間の視覚に近い処理構造

### 学習と推論

| フェーズ | 内容 |
|---|---|
| 学習 | 大量の正解付き画像からパターンを学習 |
| 推論 | 未知の画像を入力し、分類結果を出力 |

→ **推論時の入力は「完全に信用されている」前提**で設計されています。  

## Adversarial Examples(敵対的サンプル)とは

### 定義(エビデンス)

**Adversarial Example** とは、人間にはほぼ同じ画像に見えますが、AIには誤認識を引き起こすよう細工された入力データを指します。  

#### 代表的根拠

- Szegedy et al., 2013  
- NIST IR 8269(AI Security)

### 攻撃の本質

- ピクセル値を**極めて微小に改変**
- 人間の視覚では識別困難
- AIの分類結果が大きく変化

例：  

- パンダ → 人間にはパンダ  
- AI → テナガザルと誤認識

## なぜAIはだまされるのか

### 高次元空間の問題

画像は以下の特徴を持ちます。  

- 数万〜数百万次元のデータ
- 人間の感覚とは異なる距離尺度

→ **人間にとって微差でも、AIにとっては決定的差分になります**

### 線形近似の弱点

深層学習モデルは、局所的には線形近似として振る舞います。

- 勾配方向にノイズを加える
- 出力が大きく変化

これを悪用したのが敵対的サンプル攻撃です。  

## Adversarial Examples 攻撃の手法

### FGSM(Fast Gradient Sign Method)

- 勾配の符号方向に微小ノイズを付加
- 高速/簡易な攻撃

### Iterative Attack

- FGSMを反復適用
- 精度は高いが計算量増加

### Physical Adversarial Attack

- 実世界で成立する攻撃
- 標識に細工 → 自動運転が誤認識

→ **サイバー空間に限定されない点が重要**

## AI特有の攻撃手法(周辺知識)

### データポイズニング攻撃

- 学習データに不正データを混入
- モデル自体を歪める

### モデル反転攻撃(Model Inversion)

- 出力結果から学習データを推測
- 個人情報漏えいリスク

### モデル抽出攻撃(Model Extraction)

- API応答を大量取得
- 内部モデルを再現

## 対策技術

### Adversarial Training

- 敵対的サンプルを含めて学習
- 一定の耐性を獲得

### 入力検証/異常検知

- ノイズ特性の分析
- 信頼度スコアの導入

### 多層防御の必要性

- AI単体に依存しない
- 人的確認/ルールベース併用

## 従来のセキュリティ攻撃との違い

| 観点 | 従来攻撃 | Adversarial 攻撃 |
|---|---|---|
| 攻撃対象 | OS/アプリ | 学習モデル |
| 攻撃手段 | コード/通信 | 入力データ |
| 可視性 | 比較的明確 | 人間には不可視 |

